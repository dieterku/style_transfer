{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style Transfer with  a VGG-11 network\n",
    "### Basic principles of style transfer\n",
    "In this notebook we implement a style transfer method, which is originally described in the paper [Image Style Transfer Using Convolutional Neural Networks, by Gatys, Ecker and Bethge](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf). Please  have a look into this document in order to get some initial understanding. \n",
    "\n",
    "<img src='style_transfer.PNG'>\n",
    "\n",
    "Our target is to \"transfer\" the optical **style** of one image to another image by preserving the **content** of the original image. How to do that with a neural network? \n",
    "\n",
    "The basic idea is to feed a CNN (convolutional neural network) with images and to **extract** the style and content somehow. So the next question is, where the style and the content are to be found inside the network?\n",
    "\n",
    "Let's first have a short look how the human brain (visual cortex, etc.) would process some optical information step by step: First of all (when some photons are recepted in the eye) the information would be very basic - e.g. \"something red seen in the lower left corner\". Then this information gets refined while flowing through the neurons: \"something red seen, which has a round shape\". Then - at some point - we would get the information: \"a ball seen\". If we look at this facts under the aspect of **style** and **content** we see something clearly: \n",
    "\n",
    "* **style** (color, texture, etc.) is more located in the earlier layers of the neural network (both human brains and artificial CNNs) - e.g. the usage of the color red \n",
    "* **content** is more located in the deeper layers of the neural network - e.g. recognizing special objects (persons, cars, etc. - or a ball in our case)\n",
    "\n",
    "These insights are technically used for the **style transfer** via CNNs as follows:\n",
    "\n",
    "* The **style image** is send through the CNN in order to get the style information out of the earlier layers\n",
    "* The **content image** is send through the CNN in order to get the content information out of the deeper layers\n",
    "\n",
    "Next question is: How to transfer the style into the target image? We will start off with the target image as being a copy of the content image and \"**inject the style**\" with backpropagation step by step. However - there is one difference as with \"normal backpropagation\" of neural networks: Normally the loss is computed based on the final output of the network and the weights are adjusted inside the network. With style transfer the situation is different:\n",
    "\n",
    "* The **style loss** is computed at the **style layers** (early in the network)\n",
    "* The **content loss** is computed at the **content layers** (deeper in the network)\n",
    "* **Backpropagation** does not affect the weights inside the network, but the **target image** being the input of network. (In our case it is better to regard the target image as the **very first layer** of the network.)\n",
    "\n",
    "<img src='style_injection.PNG'>\n",
    "\n",
    "### Network selection\n",
    "In this notebook we are using the VGG-11 network for implementation (in contrast to the VGG-19 network, which was proposed in the original paper, [Image Style Transfer Using Convolutional Neural Networks, by Gatys](https://www.cv-foundation.org/openaccess/content_cvpr_2016/papers/Gatys_Image_Style_Transfer_CVPR_2016_paper.pdf) Feel free to use other (more complex but also slower) networks, if you are a little bit familiar with style transfer. When using other VGG networks there should only be minor changes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Technical Preconditions\n",
    "There are the following basic preconditions for running this notebook:\n",
    "* Pytorch has to be installed (see https://pytorch.org/  for the installation notes)\n",
    "* The usage of a GPU is highly recommened. (For small images also a CPU \"might do it\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import requests\n",
    "from torchvision import transforms, models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get the pre-trained VGG-11 model\n",
    "\n",
    "Pytorch offers __[pre-trained models](https://pytorch.org/docs/stable/torchvision/models.html)__, from which we will use the VGG-11 model. If a GPU is available (which should be the case) the model is transferred  to the GPU.\n",
    "The pre-trained VGG-11 model is frozen next (as mentioned before) because we will only manipulate the resulting target image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for GPU available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (3): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (4): ReLU(inplace=True)\n",
       "  (5): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (6): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (7): ReLU(inplace=True)\n",
       "  (8): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (9): ReLU(inplace=True)\n",
       "  (10): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (11): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (12): ReLU(inplace=True)\n",
       "  (13): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (14): ReLU(inplace=True)\n",
       "  (15): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (16): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (17): ReLU(inplace=True)\n",
       "  (18): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (19): ReLU(inplace=True)\n",
       "  (20): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the pre-trained VGG-11 model (only the feature part is relevant for us - not the classifier part)\n",
    "vgg = models.vgg11(pretrained=True).features \n",
    "\n",
    "# freeze all model parameters (because only the resulting target image will be manipulated)\n",
    "vgg.requires_grad_(False)\n",
    "vgg.to(device)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load content and style images\n",
    "\n",
    "Next a content and a style image are loaded. Feel free to choose your own images. (Later we will be using a copy of the content image as a starting point to \"inject\" the style.)\n",
    "\n",
    "We have choosen an image length of 400 - but huger images are no problem (as long as a GPU is available). \n",
    "The following methods `load_image()` and `image_convert()` are for bringing the content and target image to the same size and also for converting images from one format to another:\n",
    "\n",
    "* Image (as on the file system) <=> Image (numpy format) <=> Image (Pytorch-specific tensor format)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_image(img_path, max_size=400, shape=None):\n",
    "    '''Convert the image file to tensor format and resize it\n",
    "    \n",
    "    :param  :img_path Local file path \n",
    "    :param  :max_size Maximal pixel size for small side of the image\n",
    "    :param  :shape    Pixel height and width\n",
    "    :return           Tensor appropriate for network input\n",
    "    '''\n",
    "\n",
    "    image = Image.open(img_path).convert('RGB') \n",
    "    \n",
    "    # set size according to input parameters\n",
    "    if max(image.size) > max_size:\n",
    "        size = max_size\n",
    "    else:\n",
    "        size = max(image.size)\n",
    "    if shape is not None:\n",
    "        size = shape\n",
    "        \n",
    "    # resize and normalize so the image fits to the original training data of the VGG network \n",
    "    in_transform = transforms.Compose([\n",
    "        transforms.Resize(size),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.485, 0.456, 0.406), \n",
    "                             (0.229, 0.224, 0.225))])\n",
    "\n",
    "    # drop the transparency channel\n",
    "    image = in_transform(image)[:3,:,:]\n",
    "    \n",
    "    # add an additional batch dimension\n",
    "    image = image.unsqueeze(0)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# local file names\n",
    "content_file_path = 'rabbit.jpg'\n",
    "style_file_path   = 'pattern.jpg'\n",
    "\n",
    "# Maximum pixel size (small side of the image)\n",
    "max_size          = 400\n",
    "\n",
    "# load content and style image with same size\n",
    "# hint: for the content image the ratio is kept, for the style image it is not kept\n",
    "content_image = load_image(content_file_path, max_size=max_size).to(device)\n",
    "style_image   = load_image(style_file_path, shape=content_image.shape[-2:]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_convert(tensor):\n",
    "    \"\"\"Convert the tensor to an image, which can be plotted \n",
    "    \n",
    "    param: tensor: Image in tensor format\n",
    "    return:        Image in numpy format, so it can be plotted\n",
    "    \"\"\"\n",
    "    \n",
    "    image = tensor.to(\"cpu\").clone().detach()\n",
    "    \n",
    "    # Remove batch dimension first\n",
    "    image = image.numpy().squeeze() \n",
    "    \n",
    "    # Swap dimension (height and with) with color channel \n",
    "    image = image.transpose(1,2,0) \n",
    "    \n",
    "    # Denormalize (= revert previous normalization)\n",
    "    image = image * np.array((0.229, 0.224, 0.225)) + np.array((0.485, 0.456, 0.406)) \n",
    "    \n",
    "    # Make sure pixel values are not out of range\n",
    "    image = image.clip(0, 1)\n",
    "\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display both the content and the style image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 8))\n",
    "\n",
    "ax1.imshow(image_convert(content_image))\n",
    "ax2.imshow(image_convert(style_image))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Content and style features\n",
    "The function `get_features()` gets us the features - both style and content features - of an image at all layers, which we are interested in. This is done by sending any image through all layers of the network at collecting \"feature snapshots\" of certain layers (style or content relevant layers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(image, model):\n",
    "    \"\"\" Send an image forward through a model and get the features for special layers\n",
    "    \n",
    "    param: image: The image, which is send through the network\n",
    "    param: model: The neural network \n",
    "    return:       A dictionary consisting of layer names and the associated features \n",
    "    \"\"\"\n",
    "    \n",
    "    # all content and style layers of the network, which we are interested in\n",
    "    layers = {'0' : 'conv1_1',\n",
    "              '3' : 'conv2_1', \n",
    "              '6':  'conv3_1', \n",
    "              '11': 'conv4_1',\n",
    "              '13': 'conv4_2',  \n",
    "              '16': 'conv5_1'}\n",
    "        \n",
    "    # features (= output of the layers, which we are interested in)\n",
    "    features = {}\n",
    "    \n",
    "    # send the image through the network - layer by layer\n",
    "    # collect the layer output (= features) for the special layers we are interested in\n",
    "    im       = image\n",
    "    for name, layer in model._modules.items():\n",
    "\n",
    "        im = layer(im)   \n",
    "        if name in layers:\n",
    "            features[layers[name]] = im                  \n",
    "            \n",
    "    return features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gram matrix \n",
    "Now that we can extract the features via `get_features()` we still havn't got the \"style itself\" or some numbers describing the style \"in some concise way\". That's were the __[gram matrix](https://en.wikipedia.org/wiki/Gramian_matrix)__ comes into play.\n",
    "\n",
    "The basic idea is to **set each feature in relation to each other**. If we had 16 features, the gram matrix would be 16x16. The gram matrix tells us, how each combination of two features is **correlated** to each other. The correlation is computed as the scalar product of the two features (which we have flattened before to get rid of their spatial dimensions). The idea of correlations is best described by a (simplified) example:\n",
    "\n",
    "Let's say *feature 1* measures the occurrence of the color green and *feature 2* measures the occurence of diagonal lines, then a high value of *scalar product (feature 1, feature 2)* would indicate a lot of diagonal green lines, a very low value no diagonal green lines.\n",
    "\n",
    "The function `gram_matrix()` given us the gram matrix for some special layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gram_matrix(tensor):\n",
    "    \"\"\" Calculate the gram matrix  \n",
    "        see https://en.wikipedia.org/wiki/Gramian_matrix for more details\n",
    "        \n",
    "        param: tensor: Feature tensor for special CNN layer\n",
    "        return:        Gram matrix\n",
    "    \"\"\"\n",
    "    \n",
    "    # flatten each feature of the convolutional layer \n",
    "    flattened_tensor = tensor.flatten(start_dim=2).squeeze()\n",
    "    \n",
    "    # compute the gram matrix, which contains the interim relations (scalar-products) of the features of a layer\n",
    "    gram = flattened_tensor.matmul(flattened_tensor.T)\n",
    "    \n",
    "    return gram "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Get the initial content and style features\n",
    "Because we will need the initial content and style features over and over again during the loss computations, we are getting them once beforehand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the initial content and style features (before computing the target image)\n",
    "content_features = get_features(content_image, vgg)              \n",
    "style_features   = get_features(style_image, vgg)                \n",
    "\n",
    "# calculate the gram matrices for each style-relevant layer \n",
    "style_gram = {layer: gram_matrix(style_features[layer]) for layer in style_features}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the resulting target image\n",
    "For the resulting target image we start off with a copy of the content image. This image will be augmented with the style of the style image step by step. For the (style) optimization we are using an Adam optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an initial result image - as a copy of the content image - and make it changable\n",
    "result_image = content_image.clone().requires_grad_(True).to(device)   \n",
    "\n",
    "# tell the optimizer, that we are only interested in changing the result image\n",
    "lr = lr=0.003\n",
    "optimizer = optim.Adam([result_image], lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define loss functions\n",
    "\n",
    "#### Total loss\n",
    "\n",
    "The loss consists **both** of the **style** and the **content** loss, which means:\n",
    "* The **content** of the original content image should be preserved as good as possible\n",
    "* The **style** of the original style image should be \"injected\" as much as possible\n",
    "\n",
    "These two types of loss have to be balanced out by appropriate weights.\n",
    "\n",
    "#### Content loss\n",
    "The content loss is computed based on the features of the \"content layer\" - as a \"delta\" of the original content image and the currently computed target image.\n",
    "\n",
    "Hint: In this implementation we are only using one content layer (not many).\n",
    "\n",
    "#### Style loss\n",
    "\n",
    "For the style loss the situation is a bit more complicated, because there are many \"style layers\". Again we have to balance out - but now it is balancing out the weights between the different style layers: Do we favor more \"basic styles\" - e.g. colors - or do we favor more \"sophisticated styles\" (pattern, textures, etc.)? This decision has to be taken. In the implementation below it is some mixture. Feel free to change the values.\n",
    "\n",
    "The style loss is finally computed based on the features of the \"style layers\" - as a \"delta\" of the original style image and the currently computed target image. For the individual style layers we need to use a gram matrix each, to get hold of some numerical values, which describe the style."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_style_loss(result_features):\n",
    "    \"\"\"Get the style loss\n",
    "    \n",
    "    param: :result_features Features of the current computation of the result image\n",
    "    return:                 Style loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # weights for each style relevant layer \n",
    "    # prefering earlier layers will result in more basic style artifacts (e.g. colouring)\n",
    "    # prefering later layers will result in more sophisticated style artifacts (e.g. special textures)\n",
    "    style_weights = {'conv1_1': 1.,\n",
    "                     'conv2_1': 0.8,\n",
    "                     'conv3_1': 0.5,\n",
    "                     'conv4_1': 0.3,\n",
    "                     'conv5_1': 0.2}    \n",
    "\n",
    "    # iterate through each style layer and add up the style loss\n",
    "    style_loss = 0\n",
    "    for layer in style_weights:   \n",
    "        \n",
    "        # get the style features of the result and the original style image\n",
    "        result_feature = result_features[layer] \n",
    "        style_feature  = style_features[layer]\n",
    "        _, dim, height, width = result_feature.shape\n",
    "        \n",
    "        # Calculate the gram matrices\n",
    "        result_gram = gram_matrix(result_feature)                                #                     \n",
    "        style_gram  = gram_matrix(style_feature)     \n",
    "        \n",
    "        # Calculate the style loss \n",
    "        layer_style_loss = torch.mean((result_gram - style_gram)**2)\n",
    "        \n",
    "        # add up the style loss\n",
    "        style_loss += layer_style_loss / (dim * height * width)    \n",
    "    \n",
    "    return style_loss\n",
    "    \n",
    "def get_content_loss(result_features): \n",
    "    \"\"\"Get the content loss\n",
    "    \n",
    "    param: :result_features Features of the current computation of the result image\n",
    "    return:                 Content loss\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select the layer relevant for the content\n",
    "    content_layer = 'conv4_2'\n",
    "    \n",
    "    # Calculate the content loss\n",
    "    content_loss = torch.mean((result_features[content_layer] - content_features[content_layer])**2)\n",
    "    \n",
    "    return content_loss\n",
    "    \n",
    "def get_total_loss(result_features):  \n",
    "    \"\"\"Get the total loss\n",
    "    \n",
    "    param: :result_features Features of the current computation of the result image\n",
    "    return:                 Total loss (= weighted content and style loss)\n",
    "    \"\"\"\n",
    "    \n",
    "    # relative weights of content and style \n",
    "    content_weight = 1  \n",
    "    style_weight = 1e6  \n",
    "    \n",
    "    content_loss = get_content_loss(result_features)\n",
    "    style_loss   = get_style_loss(result_features)\n",
    "    \n",
    "    return content_weight * content_loss + style_weight * style_loss    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run the image computation\n",
    "Generating the resulting image - which is a fusion of style and  content - is straight forward. Decide about the iterations. A value of 6000 has proven to be sufficient in most cases. The interim result will be shown every 400 iterations, so you can directly watch the fusion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# iteration hyper parameters\n",
    "show_every = 400\n",
    "iterations = 6000  \n",
    "\n",
    "for ii in range(1, iterations+1):\n",
    "    \n",
    "    # get the features of the current computation\n",
    "    result_features = get_features(result_image, vgg)\n",
    "    \n",
    "    # compute the total loss (regarding both content and style)\n",
    "    total_loss   = get_total_loss(result_features)\n",
    "    \n",
    "    # update your result image\n",
    "    optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    optimizer.step()                                       \n",
    "    \n",
    "    # display intermediate images and print out the loss\n",
    "    if  ii % show_every == 0:\n",
    "        print('Total loss: ', total_loss.item())\n",
    "        plt.imshow(image_convert(result_image))\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display the resulting image\n",
    "Display the final result and (optionally) save as file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the content and resulting image\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(20, 10))\n",
    "ax1.imshow(image_convert(content_image))\n",
    "ax2.imshow(image_convert(result_image))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save image to the local file system\n",
    "result_file_path = 'result1.jpg'\n",
    "matplotlib.image.imsave(result_file_path, image_convert(result_image))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
